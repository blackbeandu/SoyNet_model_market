# Transformers_MaskedLM Model Overview
It is an algorithm that supports both two-way learning models for understanding natural language.
As a pre-training method, it is said to perform better than Word2Vec, GloVe, and Fasttext, the embedding method used before BERT appeared.

# Prerequisites

#### NVIDA Development Environment
 - CUDA (= 11.1)
 - cuDNN (>= 8.x)
 - TensorRT (= 8.2.1.8)
 
    â€» You need to use .dll and .so files that match CDUA and TensorRT versions. If you want another version, Please contact [SoyNet](https://soynet.io/en/).
#### S/W
 - OS : Ubuntu 18.04 LTS
 - Others : OpenCV (for reading video files and outputting the screen)


# Parameters
 - `extend_param`
      - `batch_size` : This is the batch-size of the data you want to input.
      - `engine_serialize` : Whether or not the engine is created. (default : 0)
         - 0: Load existing engine file.
         - 1 : Create engine file from weight file. you need to set value to in following cases.
            - Change extended param.
            - Change weight file.
      - `data_length` : Data Length.
      - `weight_file` : The path to weight_file.
      - `engine_file` : The path to engine_file.
      - `log_file` :  The path to log_file.
      
# Start SoyNet Demo Examples

* Clone github repository

```
$ git clone https://github.com/soynet-support/SoyNet_model_market.git
```

* download pre-trained weight files (already converted for SoyNet)

```
$ cd SoyNet_model_market/models/Transformers_MaskedLM/weights && bash ./download_weights.sh
```

* Run
```
$ cd ../../../samples
$ python Transformers_MaskedLM.py 
```

If you cannot create an engine, review the configuration settings again.

It is possible to create a C++ executable file.

Contact [SOYNET](https://soynet.io/#/contact-us).

# Reference
 - [Original Code](https://github.com/huggingface/transformers)

# Acknowlegement

Transformers MaskedLM is under Apache License. 
See License terms and condition: [License](https://github.com/huggingface/transformers/blob/main/LICENSE)
